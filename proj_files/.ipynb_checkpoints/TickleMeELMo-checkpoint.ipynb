{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "482c128d-7807-45a4-843b-7aab5ed458aa",
   "metadata": {},
   "source": [
    "# ELMo-LSA-SVM Model\n",
    "- Uses GPU if available\n",
    "- Creates pickle files for the ELMo embeddings under \"/embeddings/ELMo/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8e4e57f-aa0e-4aab-a35b-3739d8952084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 02:26:40.112599: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732904800.124714    6952 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732904800.127976    6952 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/walnuts/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## turn off warnings for cleaner execution\n",
    "import os, shutil\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "## IMPORTS\n",
    "# GLOVE Embeddings\n",
    "import spacy\n",
    "\n",
    "# ELMo\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# SVM\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, ParameterGrid, StratifiedKFold\n",
    "import scipy.stats as stats\n",
    "from joblib import parallel_backend\n",
    "\n",
    "# Preprocessing\n",
    "import csv\n",
    "import chardet\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import demoji\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import json\n",
    "\n",
    "# metrics\n",
    "from sklearn.model_selection import StratifiedKFold, learning_curve\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, balanced_accuracy_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7c8ea2f-3b8f-461c-bd1c-bffd8fce5528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "# check if gpu is being used\n",
    "num_gpu = len(tf.config.list_physical_devices('GPU'))\n",
    "print(\"Num GPUs Available: \", num_gpu)\n",
    "\n",
    "print(\"Using GPU\" if num_gpu > 0 else \"Not Using GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6af4d94e-114c-43e4-a3ab-d744fac4667f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rows: 2338\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Username</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Date posted</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@fudaishii</td>\n",
       "      <td>i'm genuinely going to attempt tonight i can't do this anymore i can't handle all this stress i wish i was never born bro</td>\n",
       "      <td>9/11/24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@yourdystiny</td>\n",
       "      <td>Becoming less reactive is a huge part of growth &amp; decreasing stress. If you let everything get you worked up, you’ll damage your mind, body &amp; spirit.</td>\n",
       "      <td>9/8/24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@ocenhxu</td>\n",
       "      <td>me ??? tired ??? stressed ??? exhausted ??? i wanna cry ??? yes.</td>\n",
       "      <td>9/8/24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@ifeelgoodto</td>\n",
       "      <td>skipping meals, irregular sleeping habits, overthinking, stress, tired and drained. that's me, that's my everyday life</td>\n",
       "      <td>9/10/24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@ysuckme</td>\n",
       "      <td>you deserve to be happy. not confused, not hurt, not stressed, just happy.</td>\n",
       "      <td>9/8/24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Username  \\\n",
       "0    @fudaishii   \n",
       "1  @yourdystiny   \n",
       "2      @ocenhxu   \n",
       "3  @ifeelgoodto   \n",
       "4      @ysuckme   \n",
       "\n",
       "                                                                                                                                                   Tweet  \\\n",
       "0                              i'm genuinely going to attempt tonight i can't do this anymore i can't handle all this stress i wish i was never born bro   \n",
       "1  Becoming less reactive is a huge part of growth & decreasing stress. If you let everything get you worked up, you’ll damage your mind, body & spirit.   \n",
       "2                                                                                       me ??? tired ??? stressed ??? exhausted ??? i wanna cry ??? yes.   \n",
       "3                                 skipping meals, irregular sleeping habits, overthinking, stress, tired and drained. that's me, that's my everyday life   \n",
       "4                                                                             you deserve to be happy. not confused, not hurt, not stressed, just happy.   \n",
       "\n",
       "  Date posted  Label  \n",
       "0     9/11/24      1  \n",
       "1      9/8/24      0  \n",
       "2      9/8/24      1  \n",
       "3     9/10/24      1  \n",
       "4      9/8/24      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset / read csv\n",
    "# MAKE SURE CSV IS IN UTF-8 (if tweets have emojis)\n",
    "\n",
    "## Testing if csv can open (Use for troubleshooting)\n",
    "# with open('Eng_Tweets.csv') as csv_file:\n",
    "#   csvFile = csv.reader(csv_file, delimiter=',')\n",
    "#   for row in csv_file:\n",
    "#     print(row)\n",
    "\n",
    "## RUN PARAMETERS\n",
    "LANG = 1  # 1 = English | 2 = Tagalog | 3 = TagLish | 4 = Mixed\n",
    "DO_GRIDSEARCH = False\n",
    "GENERATE_NEW_ELMO = True\n",
    "COMBO_METHOD = 1 # 1 = Concatenation | 2 = Averaging\n",
    "manual_params = {\n",
    "    'probability': True,\n",
    "    'C': 1,\n",
    "    'kernel': 'linear',\n",
    "    'class_weight': None,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Assign the CSV path based on LANG value\n",
    "if LANG == 1:\n",
    "    csv_path = 'Eng_Tweets.csv'\n",
    "elif LANG == 2:\n",
    "    csv_path = 'Tag_Tweets.csv'\n",
    "elif LANG == 3:\n",
    "    csv_path = 'TagLish_Tweets.csv'\n",
    "elif LANG == 4:\n",
    "    csv_path = 'Mixed_Tweets.csv'\n",
    "else:\n",
    "    raise ValueError(\"Invalid value for LANG. Must be 1, 2, 3, or 4.\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print(\"total rows: \" + str(len(df)))\n",
    "df.head() ## head won't show emojis unless using print function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4700226b-a641-4d3c-80b0-520757dba866",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "- Remove mentions (@)\n",
    "- Remove hashtags (#)\n",
    "- Remove URLs\n",
    "- Replace emojis with textual description (Using demoji)\n",
    "- Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af1ee32c-cb75-4237-9bf7-7aae54bfc298",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREPROCESSING STEPS (FOR TAGALOG)\n",
    "with open('tl_stopwords.json', 'r') as f:\n",
    "    tagalog_stop_words = json.load(f)\n",
    "\n",
    "# Preprocessing functions\n",
    "def clean_text_tl(text):\n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@[A-Za-z0-9_]+', '', text)\n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    # Replace emoji with textual descriptions\n",
    "    text = demoji.replace_with_desc(text)\n",
    "    text = re.sub(r'(:[a-zA-Z\\s]+:)', r' \\1 ', text)  # Add spaces around the shortcode\n",
    "    text = re.sub(r'(:[a-zA-Z\\s]+:)', lambda match: match.group(0).replace(' ', '_'), text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def remove_stopwords_tl(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in tagalog_stop_words]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "025fba1e-6627-477c-bc30-a5567a5690a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREPROCESSING STEPS\n",
    "def clean_text(text):\n",
    "  # Remove mentions\n",
    "  text = re.sub(r'@[A-Za-z0-9_]+', '', text)\n",
    "  # Remove hashtags\n",
    "  text = re.sub(r'#\\w+', '', text)\n",
    "  # Remove URLs\n",
    "  text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "  # Replace emoji with textual descriptions\n",
    "  text = demoji.replace_with_desc(text)\n",
    "  text = re.sub(r'(:[a-zA-Z\\s]+:)', r' \\1 ', text) # Add spaces around the shortcode\n",
    "  text = re.sub(r'(:[a-zA-Z\\s]+:)', lambda match: match.group(0).replace(' ', '_'), text)\n",
    "  text = text.strip()\n",
    "  return text\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "  words = text.split()\n",
    "  filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "  return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24f39128-a507-4168-bbcf-6b83912e0604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXECUTING ENGLISH TWEET CLEANING\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Username</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Date posted</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@fudaishii</td>\n",
       "      <td>i'm genuinely going attempt tonight can't anymore can't handle stress wish never born bro</td>\n",
       "      <td>9/11/24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@yourdystiny</td>\n",
       "      <td>Becoming less reactive huge part growth &amp; decreasing stress. let everything get worked up, you’ll damage mind, body &amp; spirit.</td>\n",
       "      <td>9/8/24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@ocenhxu</td>\n",
       "      <td>??? tired ??? stressed ??? exhausted ??? wanna cry ??? yes.</td>\n",
       "      <td>9/8/24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@ifeelgoodto</td>\n",
       "      <td>skipping meals, irregular sleeping habits, overthinking, stress, tired drained. that's me, that's everyday life</td>\n",
       "      <td>9/10/24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@ysuckme</td>\n",
       "      <td>deserve happy. confused, hurt, stressed, happy.</td>\n",
       "      <td>9/8/24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Username  \\\n",
       "0    @fudaishii   \n",
       "1  @yourdystiny   \n",
       "2      @ocenhxu   \n",
       "3  @ifeelgoodto   \n",
       "4      @ysuckme   \n",
       "\n",
       "                                                                                                                           Tweet  \\\n",
       "0                                      i'm genuinely going attempt tonight can't anymore can't handle stress wish never born bro   \n",
       "1  Becoming less reactive huge part growth & decreasing stress. let everything get worked up, you’ll damage mind, body & spirit.   \n",
       "2                                                                    ??? tired ??? stressed ??? exhausted ??? wanna cry ??? yes.   \n",
       "3                skipping meals, irregular sleeping habits, overthinking, stress, tired drained. that's me, that's everyday life   \n",
       "4                                                                                deserve happy. confused, hurt, stressed, happy.   \n",
       "\n",
       "  Date posted  Label  \n",
       "0     9/11/24      1  \n",
       "1      9/8/24      0  \n",
       "2      9/8/24      1  \n",
       "3     9/10/24      1  \n",
       "4      9/8/24      0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Execute preprocessing\n",
    "df = df.dropna()  # Get rid of NaN rows\n",
    "\n",
    "if LANG == 1:\n",
    "    print(\"EXECUTING ENGLISH TWEET CLEANING\")\n",
    "    df['Tweet'] = df['Tweet'].apply(clean_text)\n",
    "    df['Tweet'] = df['Tweet'].apply(remove_stopwords)\n",
    "elif LANG == 2:\n",
    "    print(\"EXECUTING TAGALOG TWEET CLEANING\")\n",
    "    df['Tweet'] = df['Tweet'].apply(clean_text_tl)\n",
    "    df['Tweet'] = df['Tweet'].apply(remove_stopwords_tl)\n",
    "else:\n",
    "    print(\"EXECUTING BOTH ENGLISH AND TAGALOG TWEET CLEANING\")\n",
    "    df['Tweet'] = df['Tweet'].apply(clean_text_tl)\n",
    "    df['Tweet'] = df['Tweet'].apply(remove_stopwords_tl)\n",
    "    df['Tweet'] = df['Tweet'].apply(clean_text)\n",
    "    df['Tweet'] = df['Tweet'].apply(remove_stopwords)\n",
    "    \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd68d39e-4204-41d1-8a12-3d7677f195ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0    1897\n",
       "1     441\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0fb9f1-d20a-4d79-83ab-1cefb50181bd",
   "metadata": {},
   "source": [
    "## GloVe Embeddings\n",
    "- Make sure to run \"python -m spacy download en_core_web_md\"\n",
    "- GloVe embeddings are then averaged and standardized\n",
    "- Train-Test-Validation split = 70% - 20% - 10%\n",
    "- Uses undersampling to balance the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2ba6a4c-175a-48ce-b768-6a8ce489bfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/walnuts/projects/tf-gpu/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/walnuts/projects/tf-gpu/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/walnuts/projects/tf-gpu/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/walnuts/projects/tf-gpu/lib/python3.12/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\", disable=[\"tagger\", \"parser\", \"ner\"]) # disable non word-embedding parts of spacy\n",
    "\n",
    "# Create an empty list to store the embeddings\n",
    "embeddings = []\n",
    "\n",
    "# Iterate through the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    tweet = row['Tweet']\n",
    "    doc = nlp(tweet)\n",
    "\n",
    "    # Get the embeddings for each token and store them\n",
    "    tweet_embeddings = [token.vector for token in doc]\n",
    "    embeddings.append(tweet_embeddings)\n",
    "\n",
    "# Add the embeddings to the DataFrame as a new column\n",
    "df['embeddings'] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db4baf31-82fe-4537-95ef-db42697c6fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to average embeddings for a tweet\n",
    "def average_embeddings(embeddings_list):\n",
    "    if embeddings_list:\n",
    "        return np.mean(embeddings_list, axis=0)\n",
    "    else:\n",
    "        return np.zeros(300)  # 300 = embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "620eb7d4-0807-49fb-b390-452596281e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the dataset into train, validation, and test\n",
    "def split_data(df, train_size=0.7, val_size=0.1, test_size=0.2):\n",
    "    # 70% into training 30% into validation + testing \n",
    "    train_df, temp_df = train_test_split(df, train_size=train_size, random_state=42)\n",
    "    \n",
    "    # 30% splits 10% into validation and 20% into testing\n",
    "    val_df, test_df = train_test_split(temp_df, train_size=val_size / (val_size + test_size), random_state=42)\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f23bfcee-39d5-447d-b791-208f2ce0e6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average the sentence embedding into a single entry\n",
    "df['avg_embedding'] = df['embeddings'].apply(average_embeddings)\n",
    "\n",
    "# Split the dataset\n",
    "train_df, val_df, test_df = split_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb502271-e752-473e-a8eb-11633fa93754",
   "metadata": {},
   "source": [
    "## Run ELMo in batches\n",
    "- Splits data into train-test-validate (70%-20%-10%)\n",
    "- Uses the default ELMo model\n",
    "- Max Sequence Length for embedding padding\n",
    "- Done in batches to avoid computational overload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e16df72a-407e-4306-89fa-36567e2e599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing elmo with padding\n",
    "def process_in_batches(df, batch_size=50, max_seq_length=280, pickle_dir=\"embeddings/ELMo/\"):\n",
    "    num_rows = len(df)\n",
    "    \n",
    "    # Ensure the directory exists for saving pickled files\n",
    "    os.makedirs(pickle_dir, exist_ok=True)\n",
    "    \n",
    "    # Iterate over the dataframe in chunks of batch_size\n",
    "    for start_idx in range(0, num_rows, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, num_rows)\n",
    "        batch = df[\"Tweet\"].iloc[start_idx:end_idx]\n",
    "\n",
    "        # Measure time for each batch\n",
    "        start = time.time()\n",
    "        embeddings_tensor = elmo(tf.constant(batch))[\"elmo\"]\n",
    "        end = time.time()\n",
    "        \n",
    "        length = end - start\n",
    "        print(f\"Processed rows {start_idx} to {end_idx - 1}\")\n",
    "        print(\"Time Taken for batch: \", length, \"seconds\")\n",
    "        print(\"Embedding Shape for batch: \", embeddings_tensor.shape, \"\\n\")\n",
    "        \n",
    "        # Pad all sequences in the batch to the fixed max length\n",
    "        padded_batch_embeddings = []\n",
    "        for i in range(embeddings_tensor.shape[0]):\n",
    "            seq_length = embeddings_tensor[i].shape[0]\n",
    "            padding_needed = max_seq_length - seq_length\n",
    "            # Pad the sequence with zeros (pad to match the max length)\n",
    "            padded_seq = tf.pad(embeddings_tensor[i], [[0, padding_needed], [0, 0]], mode='CONSTANT')\n",
    "            padded_batch_embeddings.append(padded_seq)\n",
    "        \n",
    "        # Stack the padded embeddings into a tensor\n",
    "        padded_batch_embeddings = tf.stack(padded_batch_embeddings)\n",
    "\n",
    "        # Save the embeddings for this batch to a pickle file\n",
    "        pickle_file = os.path.join(pickle_dir, f\"batch_{start_idx}_embeddings.pkl\")\n",
    "        with open(pickle_file, 'wb') as f:\n",
    "            pickle.dump(padded_batch_embeddings.numpy(), f)\n",
    "        \n",
    "        print(f\"Saved embeddings for batch {start_idx} to {pickle_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d52467d-7e02-4e85-9e70-45ddbecc9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkk flag for generating new elmo embeddings\n",
    "def check_and_process_data(df, batch_size, max_seq_length, pickle_dir):\n",
    "    if not GENERATE_NEW_ELMO:\n",
    "        print(f\"Skipping processing as GENERATE_NEW_ELMO is set to False.\")\n",
    "        return\n",
    "        \n",
    "    else:\n",
    "        # If the directory exists, delete it\n",
    "        if os.path.exists(pickle_dir):\n",
    "            print(f\"Directory {pickle_dir} exists. Deleting it to generate new embeddings.\")\n",
    "            shutil.rmtree(pickle_dir)\n",
    "            \n",
    "        print(f\"Processing data for {pickle_dir}...\")\n",
    "        start_total = time.time()\n",
    "        process_in_batches(df, batch_size=batch_size, max_seq_length=max_seq_length, pickle_dir=pickle_dir)\n",
    "        end_total = time.time()\n",
    "        print(f\"Total Time for {pickle_dir}: {end_total - start_total} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27940868-20a4-405e-87a2-336de468c6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading elmo model at: \"https://tfhub.dev/google/elmo/2\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load ELMo default model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading elmo model at: \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mhttps://tfhub.dev/google/elmo/2\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m elmo \u001b[38;5;241m=\u001b[39m \u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://tfhub.dev/google/elmo/2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msignatures[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded elmo model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Sequence length for padding (should be >= max tweet length in dataset)\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/tf-gpu/lib/python3.12/site-packages/tensorflow_hub/module_v2.py:100\u001b[0m, in \u001b[0;36mload\u001b[0;34m(handle, tags, options)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     99\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a string, got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m handle)\n\u001b[0;32m--> 100\u001b[0m module_path \u001b[38;5;241m=\u001b[39m \u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m is_hub_module_v1 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(_get_module_proto_path(module_path))\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tags \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_hub_module_v1:\n",
      "File \u001b[0;32m~/projects/tf-gpu/lib/python3.12/site-packages/tensorflow_hub/module_v2.py:55\u001b[0m, in \u001b[0;36mresolve\u001b[0;34m(handle)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresolve\u001b[39m(handle):\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Resolves a module handle into a path.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m  This function works both for plain TF2 SavedModels and the legacy TF1 Hub\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    A string representing the Module path.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/tf-gpu/lib/python3.12/site-packages/tensorflow_hub/registry.py:49\u001b[0m, in \u001b[0;36mMultiImplRegister.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m impl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impls):\n\u001b[1;32m     48\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mis_supported(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     fails\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mtype\u001b[39m(impl)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/projects/tf-gpu/lib/python3.12/site-packages/tensorflow_hub/compressed_module_resolver.py:81\u001b[0m, in \u001b[0;36mHttpCompressedFileResolver.__call__\u001b[0;34m(self, handle)\u001b[0m\n\u001b[1;32m     77\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_urlopen(request)\n\u001b[1;32m     78\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m resolver\u001b[38;5;241m.\u001b[39mDownloadManager(handle)\u001b[38;5;241m.\u001b[39mdownload_and_uncompress(\n\u001b[1;32m     79\u001b[0m       response, tmp_dir)\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matomic_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lock_file_timeout_sec\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/tf-gpu/lib/python3.12/site-packages/tensorflow_hub/resolver.py:421\u001b[0m, in \u001b[0;36matomic_download\u001b[0;34m(handle, download_fn, module_dir, lock_file_timeout_sec)\u001b[0m\n\u001b[1;32m    419\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading TF-Hub Module \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, handle)\n\u001b[1;32m    420\u001b[0m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mMakeDirs(tmp_dir)\n\u001b[0;32m--> 421\u001b[0m \u001b[43mdownload_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmp_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;66;03m# Write module descriptor to capture information about which module was\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# downloaded by whom and when. The file stored at the same level as a\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;66;03m# directory in order to keep the content of the 'model_dir' exactly as it\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# module caching protocol and no code in the TF-Hub library reads its\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# content.\u001b[39;00m\n\u001b[1;32m    431\u001b[0m _write_module_descriptor_file(handle, module_dir)\n",
      "File \u001b[0;32m~/projects/tf-gpu/lib/python3.12/site-packages/tensorflow_hub/compressed_module_resolver.py:78\u001b[0m, in \u001b[0;36mHttpCompressedFileResolver.__call__.<locals>.download\u001b[0;34m(handle, tmp_dir)\u001b[0m\n\u001b[1;32m     75\u001b[0m request \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_compressed_format_query(handle))\n\u001b[1;32m     77\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_urlopen(request)\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDownloadManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_uncompress\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmp_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/tf-gpu/lib/python3.12/site-packages/tensorflow_hub/resolver.py:192\u001b[0m, in \u001b[0;36mDownloadManager.download_and_uncompress\u001b[0;34m(self, fileobj, dst_path)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Streams the content for the 'fileobj' and stores the result in dst_path.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m  ValueError: Unknown object encountered inside the TAR file.\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m   \u001b[43mfile_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_tarfile_to_destination\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m   total_size_str \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39mbytes_to_readable_str(\n\u001b[1;32m    195\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_total_bytes_downloaded, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    196\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_download_progress_msg(\n\u001b[1;32m    197\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloaded \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, Total size: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url, total_size_str),\n\u001b[1;32m    198\u001b[0m       flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/projects/tf-gpu/lib/python3.12/site-packages/tensorflow_hub/file_utils.py:52\u001b[0m, in \u001b[0;36mextract_tarfile_to_destination\u001b[0;34m(fileobj, dst_path, log_function)\u001b[0m\n\u001b[1;32m     49\u001b[0m abs_target_path \u001b[38;5;241m=\u001b[39m merge_relative_path(dst_path, tarinfo\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39misfile():\n\u001b[0;32m---> 52\u001b[0m   \u001b[43mextract_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mabs_target_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39misdir():\n\u001b[1;32m     54\u001b[0m   tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mMakeDirs(abs_target_path)\n",
      "File \u001b[0;32m~/projects/tf-gpu/lib/python3.12/site-packages/tensorflow_hub/file_utils.py:35\u001b[0m, in \u001b[0;36mextract_file\u001b[0;34m(tgz, tarinfo, dst_path, buffer_size, log_function)\u001b[0m\n\u001b[1;32m     33\u001b[0m dst \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mGFile(dst_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 35\u001b[0m   buf \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m buf:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/tarfile.py:702\u001b[0m, in \u001b[0;36m_FileInFile.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[0;32m--> 702\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m     b[:\u001b[38;5;28mlen\u001b[39m(buf)] \u001b[38;5;241m=\u001b[39m buf\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buf)\n",
      "File \u001b[0;32m/usr/lib/python3.12/tarfile.py:691\u001b[0m, in \u001b[0;36m_FileInFile.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfileobj\u001b[38;5;241m.\u001b[39mseek(offset \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition \u001b[38;5;241m-\u001b[39m start))\n\u001b[0;32m--> 691\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(b) \u001b[38;5;241m!=\u001b[39m length:\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ReadError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munexpected end of data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.12/tarfile.py:528\u001b[0m, in \u001b[0;36m_Stream.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the next size number of bytes from the stream.\"\"\"\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 528\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(buf)\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m buf\n",
      "File \u001b[0;32m/usr/lib/python3.12/tarfile.py:546\u001b[0m, in \u001b[0;36m_Stream._read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 546\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m buf:\n\u001b[1;32m    548\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/http/client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/usr/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.12/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load ELMo default model\n",
    "print(\"Downloading elmo model at: \\\"https://tfhub.dev/google/elmo/2\\\"\")\n",
    "elmo = hub.load(\"https://tfhub.dev/google/elmo/2\").signatures[\"default\"]\n",
    "print(\"Loaded elmo model\")\n",
    "\n",
    "# Sequence length for padding (should be >= max tweet length in dataset)\n",
    "MAX_SEQ_LEN = 140\n",
    "\n",
    "# Assuming 'df' is the dataframe with your dataset and the column containing text is \"Tweet\"\n",
    "# train_df, val_df, test_df = split_data(df, train_size=0.7, val_size=0.1, test_size=0.2)\n",
    "\n",
    "# Directory paths for embeddings\n",
    "train_emb_dir = \"embeddings/ELMo/train/\"\n",
    "val_emb_dir = \"embeddings/ELMo/val/\"\n",
    "test_emb_dir = \"embeddings/ELMo/test/\"\n",
    "\n",
    "# Process the train, validation, and test data\n",
    "print(\"Processing train dataset...\")\n",
    "start_total = time.time()\n",
    "# process_in_batches(train_df, batch_size=50, max_seq_length=MAX_SEQ_LEN, pickle_dir=\"embeddings/ELMo/train/\")\n",
    "check_and_process_data(train_df, batch_size=50, max_seq_length=MAX_SEQ_LEN, pickle_dir=train_emb_dir)\n",
    "end_total = time.time()\n",
    "print(f\"Total Time for Training Set: {end_total - start_total} seconds\")\n",
    "\n",
    "print(\"Processing validation dataset...\")\n",
    "start_total = time.time()\n",
    "# process_in_batches(val_df, batch_size=50, max_seq_length=MAX_SEQ_LEN, pickle_dir=\"embeddings/ELMo/val/\")\n",
    "check_and_process_data(val_df, batch_size=50, max_seq_length=MAX_SEQ_LEN, pickle_dir=val_emb_dir)\n",
    "end_total = time.time()\n",
    "print(f\"Total Time for Validation Set: {end_total - start_total} seconds\")\n",
    "\n",
    "print(\"Processing test dataset...\")\n",
    "start_total = time.time()\n",
    "# process_in_batches(test_df, batch_size=50, max_seq_length=MAX_SEQ_LEN, pickle_dir=\"embeddings/ELMo/test/\")\n",
    "check_and_process_data(test_df, batch_size=50, max_seq_length=MAX_SEQ_LEN, pickle_dir=test_emb_dir)\n",
    "end_total = time.time()\n",
    "print(f\"Total Time for Test Set: {end_total - start_total} seconds\")\n",
    "\n",
    "total_time = end_total - start_total\n",
    "print(\"Generated Embeddings for entire dataset\")\n",
    "print(\"Total Time Taken: \", total_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466ff24f-3f52-423d-b017-251d6410986a",
   "metadata": {},
   "source": [
    "## Combining GloVe and ELMo Embeddings\n",
    "- Load ELMo Embeddings\n",
    "- Combine it with the GloVe Embeddings via vstack, average pooling, or Principal Component Analysis\n",
    "- Feed this combined embedding into the SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a89f4f6-a8e7-417a-b89b-a1665bfabac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_elmo_embeddings(pickle_dir, num_batches):\n",
    "def load_elmo_embeddings(pickle_dir, batch_start=0, batch_step=50, max_batches=None):\n",
    "    elmo_embeddings = []\n",
    "    batch_index = batch_start\n",
    "    batch_count = 0\n",
    "    \n",
    "    while True:\n",
    "        pickle_file = f\"{pickle_dir}/batch_{batch_index}_embeddings.pkl\"\n",
    "        \n",
    "        try:\n",
    "            # Open the pickle file\n",
    "            with open(pickle_file, 'rb') as f:\n",
    "                batch_embeddings = pickle.load(f)\n",
    "                # Average across the sequence length to get a fixed-size embedding\n",
    "                batch_avg_embeddings = np.mean(batch_embeddings, axis=1)\n",
    "                elmo_embeddings.extend(batch_avg_embeddings)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {pickle_file}. Stopping batch loading.\")\n",
    "            break\n",
    "        \n",
    "        # Update batch index\n",
    "        batch_index += batch_step\n",
    "        batch_count += 1\n",
    "        \n",
    "        # Stop if the maximum number of batches is reached\n",
    "        if max_batches is not None and batch_count >= max_batches:\n",
    "            break\n",
    "    \n",
    "    return np.array(elmo_embeddings)\n",
    "\n",
    "# Load ELMo embeddings for training data\n",
    "train_elmo_embeddings = load_elmo_embeddings(\n",
    "    pickle_dir=\"embeddings/ELMo/train\",\n",
    "    batch_start=0,    # Start from batch_0_embeddings.pkl\n",
    "    batch_step=50,    # Increment by 50 for subsequent batches\n",
    "    max_batches=None  # Load all available batches\n",
    ")\n",
    "\n",
    "# Load ELMo embeddings for validation data\n",
    "val_elmo_embeddings = load_elmo_embeddings(\n",
    "    pickle_dir=\"embeddings/ELMo/val\",\n",
    "    batch_start=0,\n",
    "    batch_step=50\n",
    ")\n",
    "\n",
    "# Load ELMo embeddings for test data\n",
    "test_elmo_embeddings = load_elmo_embeddings(\n",
    "    pickle_dir=\"embeddings/ELMo/test\",\n",
    "    batch_start=0,\n",
    "    batch_step=50\n",
    ")\n",
    "\n",
    "\n",
    "# Ensure the embeddings align with DataFrame rows\n",
    "assert len(train_elmo_embeddings) == len(train_df)\n",
    "assert len(val_elmo_embeddings) == len(val_df)\n",
    "assert len(test_elmo_embeddings) == len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ae487e-e3c7-4312-8c1d-2e2cf589ab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_elmo_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb187eba-ac69-4de5-9752-be75725c9a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_elmo_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d01c818-69d9-4b3d-b13b-fb8a2e6dbdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pad the GloVe (300) embeddings to 1024 dimensions\n",
    "def pad_glove_to_elmo(glove_embeddings, target_dim=1024):\n",
    "    if glove_embeddings.shape[0] < target_dim:\n",
    "        padding = np.zeros(target_dim - glove_embeddings.shape[0])\n",
    "        return np.concatenate([glove_embeddings, padding])\n",
    "    else:\n",
    "        return glove_embeddings\n",
    "\n",
    "# Pad GloVe embeddings to 1024 dimensions\n",
    "def average_embeddings_spacy_elmo(spacy_embeddings, elmo_embeddings):\n",
    "    # Pad Spacy embeddings to 1024 dimensions if necessary\n",
    "    padded_spacy = pad_glove_to_elmo(spacy_embeddings)\n",
    "    \n",
    "    # Now average the padded Spacy embeddings and ELMo embeddings\n",
    "    return (padded_spacy + elmo_embeddings) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64805e08-15f2-4fa9-87f7-7f8d27e8ff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = np.array(train_df['avg_embedding'].to_list())\n",
    "y_train = train_df['Label']\n",
    "# X_val = np.array(val_df['avg_embedding'].to_list())\n",
    "y_val = val_df['Label']\n",
    "# X_test = np.array(test_df['avg_embedding'].to_list())\n",
    "y_test = test_df['Label']\n",
    "\n",
    "# Concatenate SpaCy and ELMo embeddings\n",
    "if COMBO_METHOD == 1:\n",
    "    # Concatenation\n",
    "    X_train = np.hstack((np.array(train_df['avg_embedding'].to_list()), train_elmo_embeddings))\n",
    "    X_val = np.hstack((np.array(val_df['avg_embedding'].to_list()), val_elmo_embeddings))\n",
    "    X_test = np.hstack((np.array(test_df['avg_embedding'].to_list()), test_elmo_embeddings))\n",
    "else:\n",
    "    # Averaging\n",
    "    X_train = np.array([average_embeddings_spacy_elmo(spacy_emb, elmo_emb) \n",
    "                           for spacy_emb, elmo_emb in zip(train_df['avg_embedding'].to_list(), train_elmo_embeddings)])\n",
    "    \n",
    "    X_val = np.array([average_embeddings_spacy_elmo(spacy_emb, elmo_emb) \n",
    "                         for spacy_emb, elmo_emb in zip(val_df['avg_embedding'].to_list(), val_elmo_embeddings)])\n",
    "    \n",
    "    X_test = np.array([average_embeddings_spacy_elmo(spacy_emb, elmo_emb) \n",
    "                          for spacy_emb, elmo_emb in zip(test_df['avg_embedding'].to_list(), test_elmo_embeddings)])\n",
    "\n",
    "\n",
    "# UNDERSAMPLING\n",
    "# undersampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "# X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# OVERSAMPLING\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Class distribution after undersampling:\\n{pd.Series(y_train_resampled).value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7e2cd9-2e66-47c6-a1e3-33d5eb965d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train set shape: {X_train_resampled.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6970ea78-6248-4db2-8e7f-f80969ca642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b8b09b-f494-489c-8c1e-d76abfca8ce9",
   "metadata": {},
   "source": [
    "## SVM (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a8494f-08f3-4057-8d5d-eddc35581af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify param_grid for linear kernel\n",
    "if DO_GRIDSEARCH:\n",
    "    param_grid_linear = {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear'],  # Only linear kernel\n",
    "        'class_weight': [None, 'balanced'],  # Class weight\n",
    "    }\n",
    "    \n",
    "    # Stratified K Fold\n",
    "    stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Initialize the SVC model\n",
    "    svc = SVC(probability=True, random_state=42)\n",
    "    \n",
    "    # Define the GridSearchCV object with StratifiedKFold\n",
    "    grid_search_linear = GridSearchCV(\n",
    "        estimator=svc,\n",
    "        param_grid=param_grid_linear,\n",
    "        cv=stratified_kfold,\n",
    "        n_jobs=-1,\n",
    "        verbose=3,\n",
    "        scoring='roc_auc'\n",
    "    )\n",
    "    \n",
    "    # Fit the grid search to the training data\n",
    "    with parallel_backend('multiprocessing'):\n",
    "        grid_search_linear.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Get the best parameters and best score for linear kernel\n",
    "    best_params_linear = grid_search_linear.best_params_\n",
    "    best_score_linear = grid_search_linear.best_score_\n",
    "    \n",
    "    # Print the results for linear kernel\n",
    "    print(f\"Best parameters for linear kernel: {best_params_linear}\")\n",
    "    print(f\"Best cross-validation Accuracy for linear kernel: {best_score_linear:.4f}\")\n",
    "else:\n",
    "    print(\"Skipped GridSearchCV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd8366a-c99c-4dc5-836f-0d43bc4ac086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the best parameters from grid search or random search\n",
    "if DO_GRIDSEARCH:\n",
    "    model = grid_search.best_estimator_  # or \n",
    "    # model = random_search.best_estimator_\n",
    "else:\n",
    "    model = SVC(**manual_params, verbose=True)\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Final evaluation on the test set\n",
    "print(\"RESULTS:\")\n",
    "y_pred_test = model.predict(X_test)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Test Accuracy: {accuracy_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75657e7b-3603-43db-8f3e-5e271fb50879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on validation set\n",
    "# y_pred_val = best_model.predict(X_val)\n",
    "y_pred_val = model.predict(X_val)\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "precision_val = precision_score(y_val, y_pred_val)\n",
    "recall_val = recall_score(y_val, y_pred_val)\n",
    "f1_val = f1_score(y_val, y_pred_val)\n",
    "balanced_acc_val = balanced_accuracy_score(y_val, y_pred_val)  # Add balanced accuracy\n",
    "conf_matrix_val = confusion_matrix(y_val, y_pred_val)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy_val}\")\n",
    "print(f\"Validation Precision: {precision_val}\")\n",
    "print(f\"Validation Recall: {recall_val}\")\n",
    "print(f\"Validation F1 Score: {f1_val}\")\n",
    "print(f\"Validation Balanced Accuracy: {balanced_acc_val}\")  # Print balanced accuracy\n",
    "print(f\"Validation Confusion Matrix: \\n{conf_matrix_val}\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "# y_pred_test = best_model.predict(X_test)\n",
    "y_pred_test = model.predict(X_test)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "precision_test = precision_score(y_test, y_pred_test)\n",
    "recall_test = recall_score(y_test, y_pred_test)\n",
    "f1_test = f1_score(y_test, y_pred_test)\n",
    "balanced_acc_test = balanced_accuracy_score(y_test, y_pred_test)  # Add balanced accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy_test}\")\n",
    "print(f\"Test Precision: {precision_test}\")\n",
    "print(f\"Test Recall: {recall_test}\")\n",
    "print(f\"Test F1 Score: {f1_test}\")\n",
    "print(f\"Test Balanced Accuracy: {balanced_acc_test}\")  # Print balanced accuracy\n",
    "print(f\"Test Confusion Matrix: \\n{conf_matrix_test}\")\n",
    "\n",
    "# Compute ROC AUC\n",
    "# y_prob_test = best_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "y_prob_test = model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob_test)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0aac3c-183e-4c02-a745-42b12d1f905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for test set\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix_test, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Predicted Negative', 'Predicted Positive'], \n",
    "            yticklabels=['Actual Negative', 'Actual Positive'])\n",
    "plt.title('Confusion Matrix - Test Set')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Plot confusion matrix for validation set\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix_val, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Predicted Negative', 'Predicted Positive'], \n",
    "            yticklabels=['Actual Negative', 'Actual Positive'])\n",
    "plt.title('Confusion Matrix - Validation Set')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849bffc1-2a17-411e-85cc-4db0632624a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using StratifiedKFold for cross-validation\n",
    "stratified_cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    model, X_train_resampled, y_train_resampled, cv=stratified_cv,  # Use StratifiedKFold\n",
    "    scoring='accuracy', \n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calculate the mean and standard deviation of train and test scores\n",
    "train_scores_mean = train_scores.mean(axis=1)\n",
    "train_scores_std = train_scores.std(axis=1)\n",
    "test_scores_mean = test_scores.mean(axis=1)\n",
    "test_scores_std = test_scores.std(axis=1)\n",
    "\n",
    "# Print the final scores (for the largest training set size)\n",
    "print(f\"Final Training Score: {train_scores_mean[-1]:.4f}\")\n",
    "print(f\"Final Cross-validation Score: {test_scores_mean[-1]:.4f}\")\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_sizes, train_scores_mean, label='Training score', color='blue')\n",
    "plt.plot(train_sizes, test_scores_mean, label='Cross-validation score', color='green')\n",
    "\n",
    "# Fill the area between the curve and the axis to represent the standard deviation\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, color='blue', alpha=0.2)\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, color='green', alpha=0.2)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471a1eae-a189-4c68-aa52-8677c514fa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Assign the CSV path based on LANG value\n",
    "if LANG == 1:\n",
    "    path = 'English'\n",
    "elif LANG == 2:\n",
    "    path = 'Tagalog'\n",
    "elif LANG == 3:\n",
    "    path = 'Taglish'\n",
    "elif LANG == 4:\n",
    "    path = 'Mixed'\n",
    "\n",
    "# Assign the combo method COMBO_METHOD = 2 # 1 = Concatenation | 2 = Averaging\n",
    "if COMBO_METHOD == 1:\n",
    "    method = 'concatenate)_'\n",
    "elif COMBO_METHOD == 2:\n",
    "    method = 'average)_'\n",
    "\n",
    "os.makedirs('saved_models/' + path, exist_ok=True)\n",
    "\n",
    "# save the model\n",
    "model_path = 'saved_models/' + path + '/Glove-Elmo-SVM(' + method + path + '.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(\"Model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
