{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "482c128d-7807-45a4-843b-7aab5ed458aa",
   "metadata": {},
   "source": [
    "# ELMo-LSA-SVM Model\n",
    "- Uses GPU if available\n",
    "- Creates pickle files for the ELMo embeddings under \"/embeddings/ELMo/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8e4e57f-aa0e-4aab-a35b-3739d8952084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 02:41:41.598477: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732905701.611272    8205 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732905701.614617    8205 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/walnuts/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## turn off warnings for cleaner execution\n",
    "import os, shutil\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "## IMPORTS\n",
    "# GLOVE Embeddings\n",
    "import spacy\n",
    "\n",
    "# ELMo\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# SVM\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, ParameterGrid, StratifiedKFold\n",
    "import scipy.stats as stats\n",
    "from joblib import parallel_backend\n",
    "\n",
    "# Preprocessing\n",
    "import csv\n",
    "import chardet\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import demoji\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import json\n",
    "\n",
    "# metrics\n",
    "from sklearn.model_selection import StratifiedKFold, learning_curve\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, balanced_accuracy_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7c8ea2f-3b8f-461c-bd1c-bffd8fce5528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "# check if gpu is being used\n",
    "num_gpu = len(tf.config.list_physical_devices('GPU'))\n",
    "print(\"Num GPUs Available: \", num_gpu)\n",
    "\n",
    "print(\"Using GPU\" if num_gpu > 0 else \"Not Using GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6af4d94e-114c-43e4-a3ab-d744fac4667f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rows: 2338\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Username</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Date posted</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@fudaishii</td>\n",
       "      <td>i'm genuinely going to attempt tonight i can't do this anymore i can't handle all this stress i wish i was never born bro</td>\n",
       "      <td>9/11/24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@yourdystiny</td>\n",
       "      <td>Becoming less reactive is a huge part of growth &amp; decreasing stress. If you let everything get you worked up, you’ll damage your mind, body &amp; spirit.</td>\n",
       "      <td>9/8/24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@ocenhxu</td>\n",
       "      <td>me ??? tired ??? stressed ??? exhausted ??? i wanna cry ??? yes.</td>\n",
       "      <td>9/8/24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@ifeelgoodto</td>\n",
       "      <td>skipping meals, irregular sleeping habits, overthinking, stress, tired and drained. that's me, that's my everyday life</td>\n",
       "      <td>9/10/24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@ysuckme</td>\n",
       "      <td>you deserve to be happy. not confused, not hurt, not stressed, just happy.</td>\n",
       "      <td>9/8/24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Username  \\\n",
       "0    @fudaishii   \n",
       "1  @yourdystiny   \n",
       "2      @ocenhxu   \n",
       "3  @ifeelgoodto   \n",
       "4      @ysuckme   \n",
       "\n",
       "                                                                                                                                                   Tweet  \\\n",
       "0                              i'm genuinely going to attempt tonight i can't do this anymore i can't handle all this stress i wish i was never born bro   \n",
       "1  Becoming less reactive is a huge part of growth & decreasing stress. If you let everything get you worked up, you’ll damage your mind, body & spirit.   \n",
       "2                                                                                       me ??? tired ??? stressed ??? exhausted ??? i wanna cry ??? yes.   \n",
       "3                                 skipping meals, irregular sleeping habits, overthinking, stress, tired and drained. that's me, that's my everyday life   \n",
       "4                                                                             you deserve to be happy. not confused, not hurt, not stressed, just happy.   \n",
       "\n",
       "  Date posted  Label  \n",
       "0     9/11/24      1  \n",
       "1      9/8/24      0  \n",
       "2      9/8/24      1  \n",
       "3     9/10/24      1  \n",
       "4      9/8/24      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset / read csv\n",
    "# MAKE SURE CSV IS IN UTF-8 (if tweets have emojis)\n",
    "\n",
    "## Testing if csv can open (Use for troubleshooting)\n",
    "# with open('Eng_Tweets.csv') as csv_file:\n",
    "#   csvFile = csv.reader(csv_file, delimiter=',')\n",
    "#   for row in csv_file:\n",
    "#     print(row)\n",
    "\n",
    "## RUN PARAMETERS\n",
    "LANG = 1  # 1 = English | 2 = Tagalog | 3 = TagLish | 4 = Mixed\n",
    "DO_GRIDSEARCH = False\n",
    "GENERATE_NEW_ELMO = True\n",
    "COMBO_METHOD = 1 # 1 = Concatenation | 2 = Averaging\n",
    "manual_params = {\n",
    "    'probability': True,\n",
    "    'C': 1,\n",
    "    'kernel': 'linear',\n",
    "    'class_weight': None,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Assign the CSV path based on LANG value\n",
    "if LANG == 1:\n",
    "    csv_path = 'Eng_Tweets.csv'\n",
    "elif LANG == 2:\n",
    "    csv_path = 'Tag_Tweets.csv'\n",
    "elif LANG == 3:\n",
    "    csv_path = 'TagLish_Tweets.csv'\n",
    "elif LANG == 4:\n",
    "    csv_path = 'Mixed_Tweets.csv'\n",
    "else:\n",
    "    raise ValueError(\"Invalid value for LANG. Must be 1, 2, 3, or 4.\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print(\"total rows: \" + str(len(df)))\n",
    "df.head() ## head won't show emojis unless using print function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4700226b-a641-4d3c-80b0-520757dba866",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "- Remove mentions (@)\n",
    "- Remove hashtags (#)\n",
    "- Remove URLs\n",
    "- Replace emojis with textual description (Using demoji)\n",
    "- Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af1ee32c-cb75-4237-9bf7-7aae54bfc298",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREPROCESSING STEPS (FOR TAGALOG)\n",
    "with open('tl_stopwords.json', 'r') as f:\n",
    "    tagalog_stop_words = json.load(f)\n",
    "\n",
    "# Preprocessing functions\n",
    "def clean_text_tl(text):\n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@[A-Za-z0-9_]+', '', text)\n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    # Replace emoji with textual descriptions\n",
    "    text = demoji.replace_with_desc(text)\n",
    "    text = re.sub(r'(:[a-zA-Z\\s]+:)', r' \\1 ', text)  # Add spaces around the shortcode\n",
    "    text = re.sub(r'(:[a-zA-Z\\s]+:)', lambda match: match.group(0).replace(' ', '_'), text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def remove_stopwords_tl(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in tagalog_stop_words]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "025fba1e-6627-477c-bc30-a5567a5690a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREPROCESSING STEPS\n",
    "def clean_text(text):\n",
    "  # Remove mentions\n",
    "  text = re.sub(r'@[A-Za-z0-9_]+', '', text)\n",
    "  # Remove hashtags\n",
    "  text = re.sub(r'#\\w+', '', text)\n",
    "  # Remove URLs\n",
    "  text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "  # Replace emoji with textual descriptions\n",
    "  text = demoji.replace_with_desc(text)\n",
    "  text = re.sub(r'(:[a-zA-Z\\s]+:)', r' \\1 ', text) # Add spaces around the shortcode\n",
    "  text = re.sub(r'(:[a-zA-Z\\s]+:)', lambda match: match.group(0).replace(' ', '_'), text)\n",
    "  text = text.strip()\n",
    "  return text\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "  words = text.split()\n",
    "  filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "  return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24f39128-a507-4168-bbcf-6b83912e0604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXECUTING ENGLISH TWEET CLEANING\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Username</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Date posted</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@fudaishii</td>\n",
       "      <td>i'm genuinely going attempt tonight can't anymore can't handle stress wish never born bro</td>\n",
       "      <td>9/11/24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@yourdystiny</td>\n",
       "      <td>Becoming less reactive huge part growth &amp; decreasing stress. let everything get worked up, you’ll damage mind, body &amp; spirit.</td>\n",
       "      <td>9/8/24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@ocenhxu</td>\n",
       "      <td>??? tired ??? stressed ??? exhausted ??? wanna cry ??? yes.</td>\n",
       "      <td>9/8/24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@ifeelgoodto</td>\n",
       "      <td>skipping meals, irregular sleeping habits, overthinking, stress, tired drained. that's me, that's everyday life</td>\n",
       "      <td>9/10/24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@ysuckme</td>\n",
       "      <td>deserve happy. confused, hurt, stressed, happy.</td>\n",
       "      <td>9/8/24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Username  \\\n",
       "0    @fudaishii   \n",
       "1  @yourdystiny   \n",
       "2      @ocenhxu   \n",
       "3  @ifeelgoodto   \n",
       "4      @ysuckme   \n",
       "\n",
       "                                                                                                                           Tweet  \\\n",
       "0                                      i'm genuinely going attempt tonight can't anymore can't handle stress wish never born bro   \n",
       "1  Becoming less reactive huge part growth & decreasing stress. let everything get worked up, you’ll damage mind, body & spirit.   \n",
       "2                                                                    ??? tired ??? stressed ??? exhausted ??? wanna cry ??? yes.   \n",
       "3                skipping meals, irregular sleeping habits, overthinking, stress, tired drained. that's me, that's everyday life   \n",
       "4                                                                                deserve happy. confused, hurt, stressed, happy.   \n",
       "\n",
       "  Date posted  Label  \n",
       "0     9/11/24      1  \n",
       "1      9/8/24      0  \n",
       "2      9/8/24      1  \n",
       "3     9/10/24      1  \n",
       "4      9/8/24      0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Execute preprocessing\n",
    "df = df.dropna()  # Get rid of NaN rows\n",
    "\n",
    "if LANG == 1:\n",
    "    print(\"EXECUTING ENGLISH TWEET CLEANING\")\n",
    "    df['Tweet'] = df['Tweet'].apply(clean_text)\n",
    "    df['Tweet'] = df['Tweet'].apply(remove_stopwords)\n",
    "elif LANG == 2:\n",
    "    print(\"EXECUTING TAGALOG TWEET CLEANING\")\n",
    "    df['Tweet'] = df['Tweet'].apply(clean_text_tl)\n",
    "    df['Tweet'] = df['Tweet'].apply(remove_stopwords_tl)\n",
    "else:\n",
    "    print(\"EXECUTING BOTH ENGLISH AND TAGALOG TWEET CLEANING\")\n",
    "    df['Tweet'] = df['Tweet'].apply(clean_text_tl)\n",
    "    df['Tweet'] = df['Tweet'].apply(remove_stopwords_tl)\n",
    "    df['Tweet'] = df['Tweet'].apply(clean_text)\n",
    "    df['Tweet'] = df['Tweet'].apply(remove_stopwords)\n",
    "    \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd68d39e-4204-41d1-8a12-3d7677f195ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0    1897\n",
       "1     441\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0fb9f1-d20a-4d79-83ab-1cefb50181bd",
   "metadata": {},
   "source": [
    "## GloVe Embeddings\n",
    "- Make sure to run \"python -m spacy download en_core_web_md\"\n",
    "- GloVe embeddings are then averaged and standardized\n",
    "- Train-Test-Validation split = 70% - 20% - 10%\n",
    "- Uses undersampling to balance the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2ba6a4c-175a-48ce-b768-6a8ce489bfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/walnuts/projects/tf-gpu/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/walnuts/projects/tf-gpu/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/walnuts/projects/tf-gpu/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/walnuts/projects/tf-gpu/lib/python3.12/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\", disable=[\"tagger\", \"parser\", \"ner\"]) # disable non word-embedding parts of spacy\n",
    "\n",
    "# Create an empty list to store the embeddings\n",
    "embeddings = []\n",
    "\n",
    "# Iterate through the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    tweet = row['Tweet']\n",
    "    doc = nlp(tweet)\n",
    "\n",
    "    # Get the embeddings for each token and store them\n",
    "    tweet_embeddings = [token.vector for token in doc]\n",
    "    embeddings.append(tweet_embeddings)\n",
    "\n",
    "# Add the embeddings to the DataFrame as a new column\n",
    "df['embeddings'] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db4baf31-82fe-4537-95ef-db42697c6fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to average embeddings for a tweet\n",
    "def average_embeddings(embeddings_list):\n",
    "    if embeddings_list:\n",
    "        return np.mean(embeddings_list, axis=0)\n",
    "    else:\n",
    "        return np.zeros(300)  # 300 = embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "620eb7d4-0807-49fb-b390-452596281e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the dataset into train, validation, and test\n",
    "def split_data(df, train_size=0.7, val_size=0.1, test_size=0.2):\n",
    "    # 70% into training 30% into validation + testing \n",
    "    train_df, temp_df = train_test_split(df, train_size=train_size, random_state=42)\n",
    "    \n",
    "    # 30% splits 10% into validation and 20% into testing\n",
    "    val_df, test_df = train_test_split(temp_df, train_size=val_size / (val_size + test_size), random_state=42)\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f23bfcee-39d5-447d-b791-208f2ce0e6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average the sentence embedding into a single entry\n",
    "df['avg_embedding'] = df['embeddings'].apply(average_embeddings)\n",
    "\n",
    "# Split the dataset\n",
    "train_df, val_df, test_df = split_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb502271-e752-473e-a8eb-11633fa93754",
   "metadata": {},
   "source": [
    "## Run ELMo in batches\n",
    "- Splits data into train-test-validate (70%-20%-10%)\n",
    "- Uses the default ELMo model\n",
    "- Max Sequence Length for embedding padding\n",
    "- Done in batches to avoid computational overload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e16df72a-407e-4306-89fa-36567e2e599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing elmo with padding\n",
    "def process_in_batches(df, batch_size=50, max_seq_length=280, pickle_dir=\"embeddings/ELMo/\"):\n",
    "    num_rows = len(df)\n",
    "    \n",
    "    # Ensure the directory exists for saving pickled files\n",
    "    os.makedirs(pickle_dir, exist_ok=True)\n",
    "    \n",
    "    # Iterate over the dataframe in chunks of batch_size\n",
    "    for start_idx in range(0, num_rows, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, num_rows)\n",
    "        batch = df[\"Tweet\"].iloc[start_idx:end_idx]\n",
    "\n",
    "        # Measure time for each batch\n",
    "        start = time.time()\n",
    "        embeddings_tensor = elmo(tf.constant(batch))[\"elmo\"]\n",
    "        end = time.time()\n",
    "        \n",
    "        length = end - start\n",
    "        print(f\"Processed rows {start_idx} to {end_idx - 1}\")\n",
    "        print(\"Time Taken for batch: \", length, \"seconds\")\n",
    "        print(\"Embedding Shape for batch: \", embeddings_tensor.shape, \"\\n\")\n",
    "        \n",
    "        # Pad all sequences in the batch to the fixed max length\n",
    "        padded_batch_embeddings = []\n",
    "        for i in range(embeddings_tensor.shape[0]):\n",
    "            seq_length = embeddings_tensor[i].shape[0]\n",
    "            padding_needed = max_seq_length - seq_length\n",
    "            # Pad the sequence with zeros (pad to match the max length)\n",
    "            padded_seq = tf.pad(embeddings_tensor[i], [[0, padding_needed], [0, 0]], mode='CONSTANT')\n",
    "            padded_batch_embeddings.append(padded_seq)\n",
    "        \n",
    "        # Stack the padded embeddings into a tensor\n",
    "        padded_batch_embeddings = tf.stack(padded_batch_embeddings)\n",
    "\n",
    "        # Save the embeddings for this batch to a pickle file\n",
    "        pickle_file = os.path.join(pickle_dir, f\"batch_{start_idx}_embeddings.pkl\")\n",
    "        with open(pickle_file, 'wb') as f:\n",
    "            pickle.dump(padded_batch_embeddings.numpy(), f)\n",
    "        \n",
    "        print(f\"Saved embeddings for batch {start_idx} to {pickle_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d52467d-7e02-4e85-9e70-45ddbecc9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkk flag for generating new elmo embeddings\n",
    "def check_and_process_data(df, batch_size, max_seq_length, pickle_dir):\n",
    "    if not GENERATE_NEW_ELMO:\n",
    "        print(f\"Skipping processing as GENERATE_NEW_ELMO is set to False.\")\n",
    "        return\n",
    "        \n",
    "    else:\n",
    "        # If the directory exists, delete it\n",
    "        if os.path.exists(pickle_dir):\n",
    "            print(f\"Directory {pickle_dir} exists. Deleting it to generate new embeddings.\")\n",
    "            shutil.rmtree(pickle_dir)\n",
    "            \n",
    "        print(f\"Processing data for {pickle_dir}...\")\n",
    "        start_total = time.time()\n",
    "        process_in_batches(df, batch_size=batch_size, max_seq_length=max_seq_length, pickle_dir=pickle_dir)\n",
    "        end_total = time.time()\n",
    "        print(f\"Total Time for {pickle_dir}: {end_total - start_total} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27940868-20a4-405e-87a2-336de468c6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading elmo model at: \"https://tfhub.dev/google/elmo/2\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732905712.616344    8205 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3539 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded elmo model\n",
      "Processing train dataset...\n",
      "Directory embeddings/ELMo/train/ exists. Deleting it to generate new embeddings.\n",
      "Processing data for embeddings/ELMo/train/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732905719.997467    8274 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed rows 0 to 49\n",
      "Time Taken for batch:  8.801424741744995 seconds\n",
      "Embedding Shape for batch:  (50, 51, 1024) \n",
      "\n",
      "Saved embeddings for batch 0 to embeddings/ELMo/train/batch_0_embeddings.pkl\n",
      "Processed rows 50 to 99\n",
      "Time Taken for batch:  1.0367248058319092 seconds\n",
      "Embedding Shape for batch:  (50, 33, 1024) \n",
      "\n",
      "Saved embeddings for batch 50 to embeddings/ELMo/train/batch_50_embeddings.pkl\n",
      "Processed rows 100 to 149\n",
      "Time Taken for batch:  2.12335467338562 seconds\n",
      "Embedding Shape for batch:  (50, 108, 1024) \n",
      "\n",
      "Saved embeddings for batch 100 to embeddings/ELMo/train/batch_100_embeddings.pkl\n",
      "Processed rows 150 to 199\n",
      "Time Taken for batch:  0.8827493190765381 seconds\n",
      "Embedding Shape for batch:  (50, 29, 1024) \n",
      "\n",
      "Saved embeddings for batch 150 to embeddings/ELMo/train/batch_150_embeddings.pkl\n",
      "Processed rows 200 to 249\n",
      "Time Taken for batch:  1.3080182075500488 seconds\n",
      "Embedding Shape for batch:  (50, 55, 1024) \n",
      "\n",
      "Saved embeddings for batch 200 to embeddings/ELMo/train/batch_200_embeddings.pkl\n",
      "Processed rows 250 to 299\n",
      "Time Taken for batch:  0.28182101249694824 seconds\n",
      "Embedding Shape for batch:  (50, 33, 1024) \n",
      "\n",
      "Saved embeddings for batch 250 to embeddings/ELMo/train/batch_250_embeddings.pkl\n",
      "Processed rows 300 to 349\n",
      "Time Taken for batch:  0.9739375114440918 seconds\n",
      "Embedding Shape for batch:  (50, 37, 1024) \n",
      "\n",
      "Saved embeddings for batch 300 to embeddings/ELMo/train/batch_300_embeddings.pkl\n",
      "Processed rows 350 to 399\n",
      "Time Taken for batch:  1.3332457542419434 seconds\n",
      "Embedding Shape for batch:  (50, 56, 1024) \n",
      "\n",
      "Saved embeddings for batch 350 to embeddings/ELMo/train/batch_350_embeddings.pkl\n",
      "Processed rows 400 to 449\n",
      "Time Taken for batch:  1.0604887008666992 seconds\n",
      "Embedding Shape for batch:  (50, 45, 1024) \n",
      "\n",
      "Saved embeddings for batch 400 to embeddings/ELMo/train/batch_400_embeddings.pkl\n",
      "Processed rows 450 to 499\n",
      "Time Taken for batch:  0.273439884185791 seconds\n",
      "Embedding Shape for batch:  (50, 29, 1024) \n",
      "\n",
      "Saved embeddings for batch 450 to embeddings/ELMo/train/batch_450_embeddings.pkl\n",
      "Processed rows 500 to 549\n",
      "Time Taken for batch:  0.33010029792785645 seconds\n",
      "Embedding Shape for batch:  (50, 33, 1024) \n",
      "\n",
      "Saved embeddings for batch 500 to embeddings/ELMo/train/batch_500_embeddings.pkl\n",
      "Processed rows 550 to 599\n",
      "Time Taken for batch:  0.8381364345550537 seconds\n",
      "Embedding Shape for batch:  (50, 30, 1024) \n",
      "\n",
      "Saved embeddings for batch 550 to embeddings/ELMo/train/batch_550_embeddings.pkl\n",
      "Processed rows 600 to 649\n",
      "Time Taken for batch:  0.30904507637023926 seconds\n",
      "Embedding Shape for batch:  (50, 33, 1024) \n",
      "\n",
      "Saved embeddings for batch 600 to embeddings/ELMo/train/batch_600_embeddings.pkl\n",
      "Processed rows 650 to 699\n",
      "Time Taken for batch:  0.36232638359069824 seconds\n",
      "Embedding Shape for batch:  (50, 37, 1024) \n",
      "\n",
      "Saved embeddings for batch 650 to embeddings/ELMo/train/batch_650_embeddings.pkl\n",
      "Processed rows 700 to 749\n",
      "Time Taken for batch:  0.9487948417663574 seconds\n",
      "Embedding Shape for batch:  (50, 34, 1024) \n",
      "\n",
      "Saved embeddings for batch 700 to embeddings/ELMo/train/batch_700_embeddings.pkl\n",
      "Processed rows 750 to 799\n",
      "Time Taken for batch:  0.2364034652709961 seconds\n",
      "Embedding Shape for batch:  (50, 29, 1024) \n",
      "\n",
      "Saved embeddings for batch 750 to embeddings/ELMo/train/batch_750_embeddings.pkl\n",
      "Processed rows 800 to 849\n",
      "Time Taken for batch:  1.2428646087646484 seconds\n",
      "Embedding Shape for batch:  (50, 49, 1024) \n",
      "\n",
      "Saved embeddings for batch 800 to embeddings/ELMo/train/batch_800_embeddings.pkl\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/projects/tf-gpu/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/function_type_utils.py:442\u001b[0m, in \u001b[0;36mbind_function_inputs\u001b[0;34m(args, kwargs, function_type, default_values)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 442\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m \u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_defaults\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitized_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_values\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/projects/tf-gpu/lib/python3.12/site-packages/tensorflow/core/function/polymorphism/function_type.py:264\u001b[0m, in \u001b[0;36mFunctionType.bind_with_defaults\u001b[0;34m(self, args, kwargs, default_values)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns BoundArguments with default values filled in.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m bound_arguments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m/usr/lib/python3.12/inspect.py:3242\u001b[0m, in \u001b[0;36mSignature.bind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3238\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[1;32m   3239\u001b[0m \u001b[38;5;124;03mand `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;124;03mif the passed arguments can not be bound.\u001b[39;00m\n\u001b[1;32m   3241\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/inspect.py:3163\u001b[0m, in \u001b[0;36mSignature._bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   3162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m-> 3163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoo many positional arguments\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: too many positional arguments",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/projects/tf-gpu/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1179\u001b[0m, in \u001b[0;36mConcreteFunction._call_impl\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1179\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_structured_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m structured_err:\n",
      "File \u001b[0;32m~/projects/tf-gpu/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1259\u001b[0m, in \u001b[0;36mConcreteFunction._call_with_structured_signature\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Executes the wrapped function with the structured signature.\u001b[39;00m\n\u001b[1;32m   1246\u001b[0m \n\u001b[1;32m   1247\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;124;03m    of this `ConcreteFunction`.\u001b[39;00m\n\u001b[1;32m   1257\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1258\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1259\u001b[0m     \u001b[43mfunction_type_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanonicalize_function_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1260\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1261\u001b[0m )\n\u001b[1;32m   1262\u001b[0m filtered_flat_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n",
      "File \u001b[0;32m~/projects/tf-gpu/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/function_type_utils.py:422\u001b[0m, in \u001b[0;36mcanonicalize_function_inputs\u001b[0;34m(args, kwargs, function_type, default_values, is_pure)\u001b[0m\n\u001b[1;32m    421\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m _convert_variables_to_tensors(args, kwargs)\n\u001b[0;32m--> 422\u001b[0m bound_arguments \u001b[38;5;241m=\u001b[39m \u001b[43mbind_function_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_values\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bound_arguments\n",
      "File \u001b[0;32m~/projects/tf-gpu/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/function_type_utils.py:446\u001b[0m, in \u001b[0;36mbind_function_inputs\u001b[0;34m(args, kwargs, function_type, default_values)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 446\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    447\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBinding inputs to tf.function failed due to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived args: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and kwargs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msanitized_kwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for signature:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m   ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bound_arguments\n",
      "\u001b[0;31mTypeError\u001b[0m: Binding inputs to tf.function failed due to `too many positional arguments`. Received args: (<tf.Tensor: shape=(50,), dtype=string, numpy=\narray([b'feel like pure shit want find cure OCD',\n       b\"Power ain't possession, mania chained single cause, prison purpose without reflection\\xe2\\x80\\x94 unshackled, bear loss.\",\n       b\"home anymore parents lied court bring back violent mentally ill brother. Lost job depression used money lawyer. way make money remote? I'm active socials. bank balance negatives.\",\n       b'lmao teacher put class therapy appointment... gonna go 2 1st hours class skip last one',\n       b'Geez feel anxiety... Impending doom next 3 days :loudly_crying_face: :loudly_crying_face: :loudly_crying_face:',\n       b'early twenties intermingled suicidal thoughts oddly soothing',\n       b'type pain...',\n       b'Bro started mentally easing financial stress get horrible shift flat fucking tire. FUCK man',\n       b'Luckae (brother Therapy)',\n       b'Look Israel treats Palestinians Megiddo prison. different Nazi camp. torture abuse.',\n       b'started that, surprise surprise physical symptoms started manifest subsided. OCD doesn\\xe2\\x80\\x99t peak room clean. feel happier I\\xe2\\x80\\x99m letting bitterness anger consume me. Yall acting like OP telling wheelchair user walk',\n       b'think needs convo psychiatrist', b'Paranoid cult member',\n       b'Ship 20k migrants small Midwestern town eat domestic animals, overwhelm resources, force people homes: Shut racists! Ship 48 illegals elite Martha\\xe2\\x80\\x99s Vineyard: Locals revolt, Nat Guard immediately sent remove them, calls DeSantis prosecuted.',\n       b'ppl like leave famous successful ppl alone...',\n       b\"i'm genuinely going attempt tonight can't anymore can't handle stress wish never born bro\",\n       b'way gojo passed yearning onto megumi & geto gave yuuji zendaya laugh syndrome',\n       b'wanna die. want leave room anymore.', b'want die hope anymore',\n       b'therapy expensive, getting lost woods & never seen free',\n       b'suicidal myself, yet luxury stable mood either. however, burden others say \\'right things\\', especially are\\'nt cause issue, suggest ignore say, \"no, thank you. want alone,\"',\n       b\"I've told positive, friendly talkative am... always. used really shy, nervous, really lonely. MASSIVE stuttering problem, too! Music changed me, want share unlocked help unleash others! THANK awesome night!! :green_heart:\",\n       b'Mental illness real',\n       b'remember one girl hating someone reason explained saying mania episode stupid literally lost braincells',\n       b'going back therapy tomorrow, one month break nd miss there. still, think good job month give',\n       b'I\\xe2\\x80\\x99ve got used alone time now, one cares anyway',\n       b'Every time think I\\xe2\\x80\\x99m better crumble back bottom. think need grippy sock vacation :upside-down face: bad would likely lose job would make us homeless that\\xe2\\x80\\x99s really option. hope psychiatrist gets back soon \\xf0\\x9f\\xab\\xa0',\n       b'Oh wow making fun mental handicap',\n       b'man takes beautiful stray dog vet &amp; makes surgery. Providing relief pain pure kindness. :folded_hands: :film_projector: Anderson Correia',\n       b\"world missing right now: conversation Candace Owens Kanye West Jews behind world's problems including mental illness.\",\n       b'Anyone notice eyes indicate form medication legal not? What\\xe2\\x80\\x99s wrong him?',\n       b'drove 40 minutes thrift store grand opening social anxiety kick upon arrival soon got near door turned right back around got car went back home \\xf0\\x9f\\xab\\xa4',\n       b'Court clears man severe learning difficulties 1990 London murder',\n       b'use trauma manipulate victims witts end push ideology. sick similar safe effective division campaign.',\n       b'wanna comm oomf badly dont wanna overwhelm oh tragedy',\n       b\"mental breakdown alone without anyone knowing hard. we're hiding behind smiles, pretending alright. silent battles never talk speak about, praying win life\",\n       b'suicidal thoughts urges NEVER stronger',\n       b'overthink much shit hurt',\n       b\"Celebrating Beatle mania. That's fake.\",\n       b'I\\xe2\\x80\\x99ve never reassured Omicron won\\xe2\\x80\\x99t overwhelm NHS.',\n       b'Whatever seen obvious seen playing fiddle Rome burning... hope come reality shall make us feel dejected voting BJP',\n       b'try. Every damn day. Sometimes it\\xe2\\x80\\x99s enough sometimes is.',\n       b'girl know damn well you\\xe2\\x80\\x99re suicidal disbandment',\n       b'don\\xe2\\x80\\x99t want former heroin addict conspiracy theorist whose wife committed suicide finding diary detailing sexual encounters 37 different women charge mental health country. that\\xe2\\x80\\x99s happen Trump Admin.',\n       b\"edit :thumbs_up: lyrics really fit GFRIEND! Buddy, let's encourage wisely comeback instead dejected (doesn't matter whether streaming, voting, sales etc). Hwaiting\",\n       b'Bully chases kid back house',\n       b'{Amazing Comm } \\xe2\\x80\\x9cDay day succumb cannot explain Slowly fading, feeling numb vain? know? am? dont know\\xe2\\x80\\x9d',\n       b'Potential Development, AL93 Expected Form Tropical Depression within Next Days.',\n       b'Focus trump! Stay couple minutes. Hard keep dementia task!',\n       b'One day think: want die. think, quietly, actually want coffee. want nap. sandwich. book. want die turns day day want go home, want walk woods, want see friends, want sit sun. want cleaner room, want better job, want live somewhere else, want live.'],\n      dtype=object)>,) and kwargs: {} for signature: () -> Dict[['lstm_outputs1', TensorSpec(shape=(None, None, 1024), dtype=tf.float32, name=None)], ['lstm_outputs2', TensorSpec(shape=(None, None, 1024), dtype=tf.float32, name=None)], ['word_emb', TensorSpec(shape=(None, None, 512), dtype=tf.float32, name=None)], ['elmo', TensorSpec(shape=(None, None, 1024), dtype=tf.float32, name=None)], ['sequence_len', TensorSpec(shape=(None,), dtype=tf.int32, name=None)], ['default', TensorSpec(shape=(None, 1024), dtype=tf.float32, name=None)]].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m start_total \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# process_in_batches(train_df, batch_size=50, max_seq_length=MAX_SEQ_LEN, pickle_dir=\"embeddings/ELMo/train/\")\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mcheck_and_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_SEQ_LEN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_emb_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m end_total \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Time for Training Set: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_total\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_total\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 15\u001b[0m, in \u001b[0;36mcheck_and_process_data\u001b[0;34m(df, batch_size, max_seq_length, pickle_dir)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing data for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpickle_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m start_total \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 15\u001b[0m \u001b[43mprocess_in_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m end_total \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Time for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpickle_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_total\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_total\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 15\u001b[0m, in \u001b[0;36mprocess_in_batches\u001b[0;34m(df, batch_size, max_seq_length, pickle_dir)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Measure time for each batch\u001b[39;00m\n\u001b[1;32m     14\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 15\u001b[0m embeddings_tensor \u001b[38;5;241m=\u001b[39m \u001b[43melmo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melmo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     16\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     18\u001b[0m length \u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/projects/tf-gpu/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1170\u001b[0m, in \u001b[0;36mConcreteFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Executes the wrapped function.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m \n\u001b[1;32m   1123\u001b[0m \u001b[38;5;124;03m  ConcreteFunctions have two signatures:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;124;03m    TypeError: If the arguments do not match the function's signature.\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1170\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/tf-gpu/lib/python3.12/site-packages/tensorflow/python/eager/wrap_function.py:253\u001b[0m, in \u001b[0;36mWrappedFunction._call_impl\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_flat(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaptured_inputs)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 253\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/tf-gpu/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1182\u001b[0m, in \u001b[0;36mConcreteFunction._call_impl\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m structured_err:\n\u001b[1;32m   1181\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_flat_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1183\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m flat_err:\n\u001b[1;32m   1184\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(  \u001b[38;5;66;03m# pylint: disable=raise-missing-from\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;28mstr\u001b[39m(structured_err)\n\u001b[1;32m   1186\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFallback to flat signature also failed due to: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1187\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(flat_err)\n\u001b[1;32m   1188\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/tf-gpu/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1242\u001b[0m, in \u001b[0;36mConcreteFunction._call_with_flat_signature\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1238\u001b[0m       arg, (tensor_lib\u001b[38;5;241m.\u001b[39mTensor, resource_variable_ops\u001b[38;5;241m.\u001b[39mBaseResourceVariable)):\n\u001b[1;32m   1239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_signature_summary()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: expected argument \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1240\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(zero-based) to be a Tensor; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1241\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(arg)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/tf-gpu/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/projects/tf-gpu/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/projects/tf-gpu/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/tf-gpu/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/projects/tf-gpu/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load ELMo default model\n",
    "print(\"Downloading elmo model at: \\\"https://tfhub.dev/google/elmo/2\\\"\")\n",
    "elmo = hub.load(\"https://tfhub.dev/google/elmo/2\").signatures[\"default\"]\n",
    "print(\"Loaded elmo model\")\n",
    "\n",
    "# Sequence length for padding (should be >= max tweet length in dataset)\n",
    "MAX_SEQ_LEN = 140\n",
    "\n",
    "# Assuming 'df' is the dataframe with your dataset and the column containing text is \"Tweet\"\n",
    "# train_df, val_df, test_df = split_data(df, train_size=0.7, val_size=0.1, test_size=0.2)\n",
    "\n",
    "# Directory paths for embeddings\n",
    "train_emb_dir = \"embeddings/ELMo/train/\"\n",
    "val_emb_dir = \"embeddings/ELMo/val/\"\n",
    "test_emb_dir = \"embeddings/ELMo/test/\"\n",
    "\n",
    "# Process the train, validation, and test data\n",
    "print(\"Processing train dataset...\")\n",
    "start_total = time.time()\n",
    "# process_in_batches(train_df, batch_size=50, max_seq_length=MAX_SEQ_LEN, pickle_dir=\"embeddings/ELMo/train/\")\n",
    "check_and_process_data(train_df, batch_size=50, max_seq_length=MAX_SEQ_LEN, pickle_dir=train_emb_dir)\n",
    "end_total = time.time()\n",
    "print(f\"Total Time for Training Set: {end_total - start_total} seconds\")\n",
    "\n",
    "print(\"Processing validation dataset...\")\n",
    "start_total = time.time()\n",
    "# process_in_batches(val_df, batch_size=50, max_seq_length=MAX_SEQ_LEN, pickle_dir=\"embeddings/ELMo/val/\")\n",
    "check_and_process_data(val_df, batch_size=50, max_seq_length=MAX_SEQ_LEN, pickle_dir=val_emb_dir)\n",
    "end_total = time.time()\n",
    "print(f\"Total Time for Validation Set: {end_total - start_total} seconds\")\n",
    "\n",
    "print(\"Processing test dataset...\")\n",
    "start_total = time.time()\n",
    "# process_in_batches(test_df, batch_size=50, max_seq_length=MAX_SEQ_LEN, pickle_dir=\"embeddings/ELMo/test/\")\n",
    "check_and_process_data(test_df, batch_size=50, max_seq_length=MAX_SEQ_LEN, pickle_dir=test_emb_dir)\n",
    "end_total = time.time()\n",
    "print(f\"Total Time for Test Set: {end_total - start_total} seconds\")\n",
    "\n",
    "total_time = end_total - start_total\n",
    "print(\"Generated Embeddings for entire dataset\")\n",
    "print(\"Total Time Taken: \", total_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466ff24f-3f52-423d-b017-251d6410986a",
   "metadata": {},
   "source": [
    "## Combining GloVe and ELMo Embeddings\n",
    "- Load ELMo Embeddings\n",
    "- Combine it with the GloVe Embeddings via vstack, average pooling, or Principal Component Analysis\n",
    "- Feed this combined embedding into the SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a89f4f6-a8e7-417a-b89b-a1665bfabac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_elmo_embeddings(pickle_dir, num_batches):\n",
    "def load_elmo_embeddings(pickle_dir, batch_start=0, batch_step=50, max_batches=None):\n",
    "    elmo_embeddings = []\n",
    "    batch_index = batch_start\n",
    "    batch_count = 0\n",
    "    \n",
    "    while True:\n",
    "        pickle_file = f\"{pickle_dir}/batch_{batch_index}_embeddings.pkl\"\n",
    "        \n",
    "        try:\n",
    "            # Open the pickle file\n",
    "            with open(pickle_file, 'rb') as f:\n",
    "                batch_embeddings = pickle.load(f)\n",
    "                # Average across the sequence length to get a fixed-size embedding\n",
    "                batch_avg_embeddings = np.mean(batch_embeddings, axis=1)\n",
    "                elmo_embeddings.extend(batch_avg_embeddings)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {pickle_file}. Stopping batch loading.\")\n",
    "            break\n",
    "        \n",
    "        # Update batch index\n",
    "        batch_index += batch_step\n",
    "        batch_count += 1\n",
    "        \n",
    "        # Stop if the maximum number of batches is reached\n",
    "        if max_batches is not None and batch_count >= max_batches:\n",
    "            break\n",
    "    \n",
    "    return np.array(elmo_embeddings)\n",
    "\n",
    "# Load ELMo embeddings for training data\n",
    "train_elmo_embeddings = load_elmo_embeddings(\n",
    "    pickle_dir=\"embeddings/ELMo/train\",\n",
    "    batch_start=0,    # Start from batch_0_embeddings.pkl\n",
    "    batch_step=50,    # Increment by 50 for subsequent batches\n",
    "    max_batches=None  # Load all available batches\n",
    ")\n",
    "\n",
    "# Load ELMo embeddings for validation data\n",
    "val_elmo_embeddings = load_elmo_embeddings(\n",
    "    pickle_dir=\"embeddings/ELMo/val\",\n",
    "    batch_start=0,\n",
    "    batch_step=50\n",
    ")\n",
    "\n",
    "# Load ELMo embeddings for test data\n",
    "test_elmo_embeddings = load_elmo_embeddings(\n",
    "    pickle_dir=\"embeddings/ELMo/test\",\n",
    "    batch_start=0,\n",
    "    batch_step=50\n",
    ")\n",
    "\n",
    "\n",
    "# Ensure the embeddings align with DataFrame rows\n",
    "assert len(train_elmo_embeddings) == len(train_df)\n",
    "assert len(val_elmo_embeddings) == len(val_df)\n",
    "assert len(test_elmo_embeddings) == len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ae487e-e3c7-4312-8c1d-2e2cf589ab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_elmo_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb187eba-ac69-4de5-9752-be75725c9a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_elmo_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d01c818-69d9-4b3d-b13b-fb8a2e6dbdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pad the GloVe (300) embeddings to 1024 dimensions\n",
    "def pad_glove_to_elmo(glove_embeddings, target_dim=1024):\n",
    "    if glove_embeddings.shape[0] < target_dim:\n",
    "        padding = np.zeros(target_dim - glove_embeddings.shape[0])\n",
    "        return np.concatenate([glove_embeddings, padding])\n",
    "    else:\n",
    "        return glove_embeddings\n",
    "\n",
    "# Pad GloVe embeddings to 1024 dimensions\n",
    "def average_embeddings_spacy_elmo(spacy_embeddings, elmo_embeddings):\n",
    "    # Pad Spacy embeddings to 1024 dimensions if necessary\n",
    "    padded_spacy = pad_glove_to_elmo(spacy_embeddings)\n",
    "    \n",
    "    # Now average the padded Spacy embeddings and ELMo embeddings\n",
    "    return (padded_spacy + elmo_embeddings) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64805e08-15f2-4fa9-87f7-7f8d27e8ff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = np.array(train_df['avg_embedding'].to_list())\n",
    "y_train = train_df['Label']\n",
    "# X_val = np.array(val_df['avg_embedding'].to_list())\n",
    "y_val = val_df['Label']\n",
    "# X_test = np.array(test_df['avg_embedding'].to_list())\n",
    "y_test = test_df['Label']\n",
    "\n",
    "# Concatenate SpaCy and ELMo embeddings\n",
    "if COMBO_METHOD == 1:\n",
    "    # Concatenation\n",
    "    X_train = np.hstack((np.array(train_df['avg_embedding'].to_list()), train_elmo_embeddings))\n",
    "    X_val = np.hstack((np.array(val_df['avg_embedding'].to_list()), val_elmo_embeddings))\n",
    "    X_test = np.hstack((np.array(test_df['avg_embedding'].to_list()), test_elmo_embeddings))\n",
    "else:\n",
    "    # Averaging\n",
    "    X_train = np.array([average_embeddings_spacy_elmo(spacy_emb, elmo_emb) \n",
    "                           for spacy_emb, elmo_emb in zip(train_df['avg_embedding'].to_list(), train_elmo_embeddings)])\n",
    "    \n",
    "    X_val = np.array([average_embeddings_spacy_elmo(spacy_emb, elmo_emb) \n",
    "                         for spacy_emb, elmo_emb in zip(val_df['avg_embedding'].to_list(), val_elmo_embeddings)])\n",
    "    \n",
    "    X_test = np.array([average_embeddings_spacy_elmo(spacy_emb, elmo_emb) \n",
    "                          for spacy_emb, elmo_emb in zip(test_df['avg_embedding'].to_list(), test_elmo_embeddings)])\n",
    "\n",
    "\n",
    "# UNDERSAMPLING\n",
    "# undersampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "# X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# OVERSAMPLING\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Class distribution after undersampling:\\n{pd.Series(y_train_resampled).value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7e2cd9-2e66-47c6-a1e3-33d5eb965d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train set shape: {X_train_resampled.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6970ea78-6248-4db2-8e7f-f80969ca642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b8b09b-f494-489c-8c1e-d76abfca8ce9",
   "metadata": {},
   "source": [
    "## SVM (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a8494f-08f3-4057-8d5d-eddc35581af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify param_grid for linear kernel\n",
    "if DO_GRIDSEARCH:\n",
    "    param_grid_linear = {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear'],  # Only linear kernel\n",
    "        'class_weight': [None, 'balanced'],  # Class weight\n",
    "    }\n",
    "    \n",
    "    # Stratified K Fold\n",
    "    stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Initialize the SVC model\n",
    "    svc = SVC(probability=True, random_state=42)\n",
    "    \n",
    "    # Define the GridSearchCV object with StratifiedKFold\n",
    "    grid_search_linear = GridSearchCV(\n",
    "        estimator=svc,\n",
    "        param_grid=param_grid_linear,\n",
    "        cv=stratified_kfold,\n",
    "        n_jobs=-1,\n",
    "        verbose=3,\n",
    "        scoring='roc_auc'\n",
    "    )\n",
    "    \n",
    "    # Fit the grid search to the training data\n",
    "    with parallel_backend('multiprocessing'):\n",
    "        grid_search_linear.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Get the best parameters and best score for linear kernel\n",
    "    best_params_linear = grid_search_linear.best_params_\n",
    "    best_score_linear = grid_search_linear.best_score_\n",
    "    \n",
    "    # Print the results for linear kernel\n",
    "    print(f\"Best parameters for linear kernel: {best_params_linear}\")\n",
    "    print(f\"Best cross-validation Accuracy for linear kernel: {best_score_linear:.4f}\")\n",
    "else:\n",
    "    print(\"Skipped GridSearchCV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd8366a-c99c-4dc5-836f-0d43bc4ac086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the best parameters from grid search or random search\n",
    "if DO_GRIDSEARCH:\n",
    "    model = grid_search.best_estimator_  # or \n",
    "    # model = random_search.best_estimator_\n",
    "else:\n",
    "    model = SVC(**manual_params, verbose=True)\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Final evaluation on the test set\n",
    "print(\"RESULTS:\")\n",
    "y_pred_test = model.predict(X_test)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Test Accuracy: {accuracy_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75657e7b-3603-43db-8f3e-5e271fb50879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on validation set\n",
    "# y_pred_val = best_model.predict(X_val)\n",
    "y_pred_val = model.predict(X_val)\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "precision_val = precision_score(y_val, y_pred_val)\n",
    "recall_val = recall_score(y_val, y_pred_val)\n",
    "f1_val = f1_score(y_val, y_pred_val)\n",
    "balanced_acc_val = balanced_accuracy_score(y_val, y_pred_val)  # Add balanced accuracy\n",
    "conf_matrix_val = confusion_matrix(y_val, y_pred_val)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy_val}\")\n",
    "print(f\"Validation Precision: {precision_val}\")\n",
    "print(f\"Validation Recall: {recall_val}\")\n",
    "print(f\"Validation F1 Score: {f1_val}\")\n",
    "print(f\"Validation Balanced Accuracy: {balanced_acc_val}\")  # Print balanced accuracy\n",
    "print(f\"Validation Confusion Matrix: \\n{conf_matrix_val}\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "# y_pred_test = best_model.predict(X_test)\n",
    "y_pred_test = model.predict(X_test)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "precision_test = precision_score(y_test, y_pred_test)\n",
    "recall_test = recall_score(y_test, y_pred_test)\n",
    "f1_test = f1_score(y_test, y_pred_test)\n",
    "balanced_acc_test = balanced_accuracy_score(y_test, y_pred_test)  # Add balanced accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy_test}\")\n",
    "print(f\"Test Precision: {precision_test}\")\n",
    "print(f\"Test Recall: {recall_test}\")\n",
    "print(f\"Test F1 Score: {f1_test}\")\n",
    "print(f\"Test Balanced Accuracy: {balanced_acc_test}\")  # Print balanced accuracy\n",
    "print(f\"Test Confusion Matrix: \\n{conf_matrix_test}\")\n",
    "\n",
    "# Compute ROC AUC\n",
    "# y_prob_test = best_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "y_prob_test = model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob_test)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0aac3c-183e-4c02-a745-42b12d1f905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for test set\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix_test, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Predicted Negative', 'Predicted Positive'], \n",
    "            yticklabels=['Actual Negative', 'Actual Positive'])\n",
    "plt.title('Confusion Matrix - Test Set')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Plot confusion matrix for validation set\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix_val, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Predicted Negative', 'Predicted Positive'], \n",
    "            yticklabels=['Actual Negative', 'Actual Positive'])\n",
    "plt.title('Confusion Matrix - Validation Set')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849bffc1-2a17-411e-85cc-4db0632624a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using StratifiedKFold for cross-validation\n",
    "stratified_cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    model, X_train_resampled, y_train_resampled, cv=stratified_cv,  # Use StratifiedKFold\n",
    "    scoring='accuracy', \n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calculate the mean and standard deviation of train and test scores\n",
    "train_scores_mean = train_scores.mean(axis=1)\n",
    "train_scores_std = train_scores.std(axis=1)\n",
    "test_scores_mean = test_scores.mean(axis=1)\n",
    "test_scores_std = test_scores.std(axis=1)\n",
    "\n",
    "# Print the final scores (for the largest training set size)\n",
    "print(f\"Final Training Score: {train_scores_mean[-1]:.4f}\")\n",
    "print(f\"Final Cross-validation Score: {test_scores_mean[-1]:.4f}\")\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_sizes, train_scores_mean, label='Training score', color='blue')\n",
    "plt.plot(train_sizes, test_scores_mean, label='Cross-validation score', color='green')\n",
    "\n",
    "# Fill the area between the curve and the axis to represent the standard deviation\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, color='blue', alpha=0.2)\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, color='green', alpha=0.2)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471a1eae-a189-4c68-aa52-8677c514fa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "os.makedirs('saved_models/' + path, exist_ok=True)\n",
    "\n",
    "# save the model\n",
    "model_path = 'saved_models/' + path + '/Glove-Elmo-SVM(' + method + path + '.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(\"Model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
